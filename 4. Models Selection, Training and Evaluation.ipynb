{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151edd59",
   "metadata": {},
   "source": [
    "### Example:\n",
    "- Which models use for training\n",
    "- Comparison of selected models\n",
    "- How is model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e685fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91496c",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b06b4",
   "metadata": {},
   "source": [
    "# XGBoost Model\n",
    "1. More Data Preprocessing for this model:\n",
    "\n",
    "- The target variable (price) is log-transformed to reduce skewness.\n",
    "- Categorical variables are one-hot encoded.\n",
    "- Numerical features are scaled to standardize their values.\n",
    "\n",
    "2. Model Setup:\n",
    "\n",
    "- The data is split into training and testing sets to evaluate model performance.\n",
    "- An XGBoost regressor is used to predict the log-transformed price.\n",
    "\n",
    "3. Hyperparameter Tuning:\n",
    "\n",
    "- A range of hyperparameters for the model is tested using RandomizedSearchCV to find the best configuration.\n",
    "\n",
    "4. Model Evaluation:\n",
    "\n",
    "- The best model is trained, and performance is evaluated using RMSE (Root Mean Squared Error) and R² (coefficient of determination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac67eadc-53cd-4f33-994c-1cc74f5bc781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Best Parameters: {'subsample': 1.0, 'reg_lambda': 0, 'reg_alpha': 0.5, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 0.2, 'colsample_bytree': 0.8}\n",
      "Cross-Validation MSE: 0.5879523178720912\n",
      "Optimized XGBoost RMSE: 0.7628190646895546\n",
      "Optimized XGBoost R²: 0.09236845677041161\n"
     ]
    }
   ],
   "source": [
    "# Load the merged dataset\n",
    "processed_df = pd.read_csv('merged_tourism_data.csv')\n",
    "\n",
    "# Log transform 'price' to reduce skewness\n",
    "processed_df['log_price'] = np.log1p(processed_df['price'])\n",
    "\n",
    "# Drop original price column and keep log_price as target\n",
    "processed_df.drop(columns=['price'], inplace=True)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "processed_df_encoded = pd.get_dummies(processed_df, drop_first=True)\n",
    "\n",
    "# Feature and target variables\n",
    "X = processed_df_encoded.drop('log_price', axis=1)  # Features\n",
    "y = processed_df_encoded['log_price']  # Target (log-transformed price)\n",
    "\n",
    "# Apply feature scaling to numerical features (e.g., 'Tourists')\n",
    "numerical_features = ['Tourists']\n",
    "scaler = StandardScaler()\n",
    "X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_dist = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgboost_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV to find best parameters\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgboost_model, param_distributions=param_dist,\n",
    "    n_iter=100,  # Testing 100 random combinations\n",
    "    cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the randomized search to a sample of the dataset to speed up tuning\n",
    "sample_df = processed_df_encoded.sample(n=20000, random_state=42)\n",
    "X_sample = sample_df.drop('log_price', axis=1)\n",
    "y_sample = sample_df['log_price']\n",
    "random_search.fit(X_sample, y_sample)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Reinitialize the XGBoost model with the best found parameters\n",
    "best_xgboost = XGBRegressor(**random_search.best_params_, objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Use cross-validation to tune the model (this replaces RandomizedSearchCV)\n",
    "cv_scores = cross_val_score(best_xgboost, X_train, y_train, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Print the average cross-validation score\n",
    "print(f\"Cross-Validation MSE: {-cv_scores.mean()}\")\n",
    "\n",
    "# Train the best model without early stopping (for now)\n",
    "best_xgboost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_xgboost.predict(X_test)\n",
    "\n",
    "# Calculate RMSE and R² for the best model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Optimized XGBoost RMSE: {rmse}\")\n",
    "print(f\"Optimized XGBoost R²: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fc23e",
   "metadata": {},
   "source": [
    "# Random Forest Model\n",
    "1. Data Preprocessing specific for this model\n",
    "\n",
    "- The dataset is encoded using one-hot encoding for categorical variables.\n",
    "- The features (X) and target (y) are defined, with price as the target variable.\n",
    "\n",
    "2. Data Split:\n",
    "\n",
    "- The data is split into training (80%) and test (20%) sets for model evaluation.\n",
    "- A sample of 10% of the training data is used for faster hyperparameter tuning.\n",
    "\n",
    "3. Hyperparameter Tuning:\n",
    "\n",
    "- A wide range of hyperparameters, such as the number of trees (n_estimators), tree depth (max_depth), and more, is tested using RandomizedSearchCV.\n",
    "- The best hyperparameters are selected based on negative mean squared error (MSE).\n",
    "\n",
    "4. Model Training and Evaluation:\n",
    "\n",
    "- The best Random Forest model is trained on the full training data.\n",
    "- Predictions are made on the test set, and performance is evaluated using RMSE (Root Mean Squared Error) and R² (coefficient of determination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12bbf6b-f3f4-485b-86ee-44b9b91a6934",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m\n\u001b[1;32m     18\u001b[0m rf_param_dist \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1000\u001b[39m],\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m15\u001b[39m],  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m] \n\u001b[1;32m     25\u001b[0m }\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Perform RandomizedSearchCV for Random Forest tuning with a larger hyperparameter space\u001b[39;00m\n\u001b[1;32m     28\u001b[0m rf_random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m---> 29\u001b[0m     estimator\u001b[38;5;241m=\u001b[39m\u001b[43mrf_model\u001b[49m, param_distributions\u001b[38;5;241m=\u001b[39mrf_param_dist,\n\u001b[1;32m     30\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Fit the randomized search to a sample of the dataset to speed up tuning\u001b[39;00m\n\u001b[1;32m     34\u001b[0m rf_random_search\u001b[38;5;241m.\u001b[39mfit(X_sample, y_sample)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rf_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('merged_tourism_data.csv')\n",
    "\n",
    "# Apply one-hot encoding to categorical columns (if any)\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_encoded.drop(columns=['price']) \n",
    "y = df_encoded['price'] \n",
    "\n",
    "# Split data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Sample data\n",
    "X_sample, _, y_sample, _ = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Expand the hyperparameter grid for tuning\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 500, 1000],\n",
    "    'max_depth': [3, 5, 7, 10, 12, 15],  \n",
    "    'max_features': ['sqrt', 'log2', None, 0.7, 0.8], \n",
    "    'min_samples_split': [2, 5, 10, 15], \n",
    "    'min_samples_leaf': [1, 2, 4, 6], \n",
    "    'bootstrap': [True, False] \n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV for Random Forest tuning with a larger hyperparameter space\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model, param_distributions=rf_param_dist,\n",
    "    n_iter=100, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the randomized search to a sample of the dataset to speed up tuning\n",
    "rf_random_search.fit(X_sample, y_sample)\n",
    "\n",
    "# Best parameters found for Random Forest\n",
    "print(\"Random Forest Best Parameters:\", rf_random_search.best_params_)\n",
    "\n",
    "# Reinitialize the Random Forest model with the best found parameters\n",
    "best_rf_model = RandomForestRegressor(**rf_random_search.best_params_, random_state=42)\n",
    "\n",
    "# Cross-validation for Random Forest with the best parameters\n",
    "rf_cv_scores = cross_val_score(best_rf_model, X_train, y_train, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Print the average cross-validation score\n",
    "print(f\"Random Forest Cross-Validation MSE: {-rf_cv_scores.mean()}\")\n",
    "\n",
    "# Train the best Random Forest model\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE and R² for Random Forest\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_y_pred))\n",
    "rf_r2 = r2_score(y_test, rf_y_pred)\n",
    "\n",
    "# Print results for Random Forest\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "print(f\"Random Forest R²: {rf_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d88a58e",
   "metadata": {},
   "source": [
    "# Linear Regression (3 types)\n",
    "1. Data Preprocessing\n",
    "\n",
    "- One-hot Encoding: Categorical columns are transformed using one-hot encoding, so the model can handle them numerically.\n",
    "- Feature Scaling: The data is scaled using StandardScaler to ensure features with different scales don't affect the model performance.\n",
    "\n",
    "2. Model Setup\n",
    "\n",
    "2. Testing three regularized linear models:\n",
    "\n",
    "- Ridge Regression: Adds an L2 penalty (square of the magnitude of coefficients), which helps prevent overfitting.\n",
    "- Lasso Regression: Adds an L1 penalty (absolute value of the coefficients), which can shrink some coefficients to zero and perform feature selection.\n",
    "- ElasticNet Regression: A combination of Lasso and Ridge, balancing both L1 and L2 penalties.\n",
    "\n",
    "3. Cross-Validation\n",
    "\n",
    "- Cross-validation is used to assess the performance of each model and prevent overfitting.\n",
    "- Mean Squared Error (MSE) is used as the evaluation metric, where lower values indicate better performance.\n",
    "\n",
    "4. Model Training and Evaluation\n",
    "- After cross-validation, the models are trained on the full training set, and predictions are made on the test set.\n",
    "Performance metrics like RMSE (Root Mean Squared Error) and R² (coefficient of determination) are calculated for each model to evaluate how well the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd68d7-122f-45e8-a4c8-7fe1e51ccd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('merged_tourism_data.csv')\n",
    "\n",
    "# Apply one-hot encoding to categorical columns (if any)\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_encoded.drop(columns=['price'])\n",
    "y = df_encoded['price']\n",
    "\n",
    "# Split data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Try Ridge, Lasso, and ElasticNet regression\n",
    "ridge_model = Ridge(alpha=5.0)\n",
    "lasso_model = Lasso(alpha=0.1, max_iter=10000)\n",
    "elasticnet_model = ElasticNet(alpha=0.1, l1_ratio=0.7, max_iter=10000)  # ElasticNet combines Lasso and Ridge\n",
    "\n",
    "# Cross-validation for Ridge\n",
    "ridge_cv_scores = cross_val_score(ridge_model, X_train_scaled, y_train, cv=3, scoring='neg_mean_squared_error')\n",
    "# Cross-validation for Lasso\n",
    "lasso_cv_scores = cross_val_score(lasso_model, X_train_scaled, y_train, cv=3, scoring='neg_mean_squared_error')\n",
    "# Cross-validation for ElasticNet\n",
    "elasticnet_cv_scores = cross_val_score(elasticnet_model, X_train_scaled, y_train, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print the average cross-validation scores\n",
    "print(f\"Ridge Cross-Validation MSE: {-ridge_cv_scores.mean()}\")\n",
    "print(f\"Lasso Cross-Validation MSE: {-lasso_cv_scores.mean()}\")\n",
    "print(f\"ElasticNet Cross-Validation MSE: {-elasticnet_cv_scores.mean()}\")\n",
    "\n",
    "# Train Ridge, Lasso, and ElasticNet models\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "elasticnet_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "ridge_y_pred = ridge_model.predict(X_test_scaled)\n",
    "lasso_y_pred = lasso_model.predict(X_test_scaled)\n",
    "elasticnet_y_pred = elasticnet_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate RMSE and R² for Ridge, Lasso, and ElasticNet\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_y_pred))\n",
    "ridge_r2 = r2_score(y_test, ridge_y_pred)\n",
    "\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_y_pred))\n",
    "lasso_r2 = r2_score(y_test, lasso_y_pred)\n",
    "\n",
    "elasticnet_rmse = np.sqrt(mean_squared_error(y_test, elasticnet_y_pred))\n",
    "elasticnet_r2 = r2_score(y_test, elasticnet_y_pred)\n",
    "\n",
    "# Print results for Ridge, Lasso, and ElasticNet\n",
    "print(f\"Ridge RMSE: {ridge_rmse}\")\n",
    "print(f\"Ridge R²: {ridge_r2}\")\n",
    "\n",
    "print(f\"Lasso RMSE: {lasso_rmse}\")\n",
    "print(f\"Lasso R²: {lasso_r2}\")\n",
    "\n",
    "print(f\"ElasticNet RMSE: {elasticnet_rmse}\")\n",
    "print(f\"ElasticNet R²: {elasticnet_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c0163",
   "metadata": {},
   "source": [
    "# Comparison of Selected Models\n",
    "## XGBoost\n",
    "\n",
    "- The XGBoost model had its hyperparameters tuned, but the performance was still pretty low, with a small R² (around 0.09).\n",
    "- Why? XGBoost can handle complex problems, but this one might have too many factors affecting the price that the model just couldn’t capture well. Some important features might also be missing or need better engineering.\n",
    "\n",
    "## Random Forest:\n",
    "\n",
    "- The Random Forest model also had the best parameters tuned but still gave a really low R² (close to 0).\n",
    "- Why? Random Forest might not be able to handle this data well because it’s set to shallow trees (max_depth=3). It also needs a lot of diverse data to learn from, and our dataset might not have enough of that variety.\n",
    "\n",
    "## Ridge, Lasso, and ElasticNet\n",
    "- All three of these models performed similarly, with high RMSE values (around 11,900).\n",
    "- Why? These models are linear, which means they might be too simple for this problem. Prices depend on many non-linear factors like holidays and tourism trends, which these models can't capture very well.\n",
    "\n",
    "# How is Model Performance?\n",
    "Even though we tried multiple models and fine-tuned them, the results were still not great. Here’s why:\n",
    "- Problem Complexity: Predicting prices on Airbnb is complicated. It’s influenced by a lot of things like holidays, the time of year, and local events, which the models might not fully capture.\n",
    "- Feature Engineering: We did some basic data cleaning, but we might not have considered all the important features or done enough transformations to really improve predictions. For example, knowing the specific area of Bangkok or the type of tourist could help.\n",
    "- Data: The datasets we have might not be large or detailed enough to get good results.\n",
    "\n",
    "## Really it comes down to time constraint and not being able to find a good dataset and question to solve. This was the best we could do. However you have mentioned before that the process is more important than the result. Despite the poor accuraries, we did think we follow the processes well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
